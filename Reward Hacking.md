#LLM 

During the [[Reinforcement Learning from Human Feedback (RLHF)]], the fine-tuned LLM might "cheat" in order to get high reward completions, but those completions might have low quality. For example, in the figure below, to get low toxicity, the RL-updated LLM generated "Beautiful love and world peace all around" which has low toxicity but has nothing to do with the prompt. This is called Reward Hacking.
![[reward_hacking_1.png]]
To avoid reward hacking, we can use a reference model to gauge the RL-updated LLM. We can calculate a KL divergence ([[Common Loss Function Glossary#Kullback-Leibler (KL) Divergence]]) shift penalty between the reference model and RL-updated LLM, and add this to the reward generated by the reward model. In this way, we keep a balance that the completion shouldn't be too far from the original model, but also more aligned to human feedback.
![[reward_hacking_3.png]]