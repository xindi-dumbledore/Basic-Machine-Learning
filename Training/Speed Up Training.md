#neural_network
- Normalizing training sets
	- Intuition: if we have a dataset with two features, and they are in different range, without normalization, the cost function is less symmetric, and it will make gradient descent harder to find minimum
![[data_normalizing.png]]
- Deal with [[Vanishing and Exploding Gradients]]
- Advanced optimizers
	- [[Gradient Descent with Momentum]]
	- [[RMSprop]]
	- [[Adam Optimizer]]
	- [[Learning Rate Decay]]
## References
1. Normalizing training sets: https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=9