## What it is
Position bias: Top items receive more clicks only because of their position?

A list of items are served to users for search or recommendation. Items at a higher rank are more likely to be clicked by users. Items at the bottom have a smaller chance to be viewed by users at all, hence will organically have fewer clicks.

## How to offset position bias
### Method 1: Inverse Propensity Weighting
Click probability is the combination of two independent variables:
- bias: the probability of clicking on a specific position in the list $P_p$
- relevance: the importance of the item $R_i$
Assumption: observed click probability on a position $P_{i, p} = R_i \times P_p$
Therefore, if we can estimate $P_p$, we can get $R_i$ from $P_{i, p}$
How to estimate $P_p$? Needs random shuffling

Cons: would deteriorate system performance

## Method 2: Position-Aware Learning (PAL)
A second method is to include position information in the model training.
![[Screenshot 2025-04-01 at 10.55.05.png]]
In this way, the ranking ML model will detect how position affects relevance. During inference, we zero out this feature by replacing them with constants.

How to choose the constants: there are no clear answers on this, but experiments suggested don't use too high rank since there are more noise.

As shown from the figure above, we can use this idea for other biased factors such as popularity.
## Other bias in the ranking model
- Presentation bias: e.g. in grid layout, items on position 4 (right under top one) may receive more clicks than item 3. To resolve this, we can add row and column information during training as the biased factors.
- Model bias: when you train an ML model on historical data generated by the same model. We can introduce a "number of clicks" feature as the biased factors.

## References
https://www.kdnuggets.com/2023/03/dealing-position-bias-recommendations-search.html