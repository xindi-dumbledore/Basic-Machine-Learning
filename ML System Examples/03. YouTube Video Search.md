## Description
User enters text into the search box, and the system displays the most relevant videos for the given text.
## Clarifying Requirements
- Is the query text only?
- What information of the video we can use? Video itself, some meta data (tags, title, description, popularity etc.)
- What kind of data is available? <video, text query>
- How many videos are there on the platform? One billion videos
- Do we need to personalized results for users? No

## Frame it as an ML task
- ML objective: retrieve videos that are most relevant to the text query, rank them from most relevant to least relevant.
- I/O: input - user query, output - list of videos ranked from most relevant to least relevant
- ML category
	- Since we are using both the video content and the video metadata, we can have two search system: visual search and text search, then fuse them together to generate the final result
	- Visual search: use to encoders to transfer query and video into embeddings, then we can calculate the similarity between the embeddings ![[Screenshot 2025-04-01 at 13.15.29.png]]
	- Text search: We can use inverted index techniques for text search. A popular choice is to use Elastic Search. For this training is not needed.
## Data
### What
- Our data are <query, video> pairs which looks something like this
	- query, video id
### Preparation
#### Query processing
We need to convert text queries to numerical values. To do that, see [[Text Preprocessing]] and [[Vocabulary & Feature Extraction]]. We can perform:
- text normalization: remove stop words, lemmatization and stemming
- tokenization
- token to ID
#### Video processing
![[Screenshot 2025-04-02 at 08.43.46.png]]
## Model details
### Text encoder
For text encoder, we can use simple statistical method (BoW, TF-IDF), more advanced methods like [[Word2Vec]] and [[Transformer]] to convert query into embeddings. Statistical method are fast but didn't take much contextual and semantic information. Word2Vec is more advanced but will produce same embedding for the same word under different context. Transformers are more powerful and more suited to this task. For more information, see [[Vocabulary & Feature Extraction]].

### Video encoder
There are two architectural options for video encoder:
- video-level models: where we process a whole video to create an embedding. This is usually based on 3D convolutions or Transformers.
- frame-level models
	- We first preprocess the video and sample some frames
	- We run the model on sampled frames to create frame embeddings
	- We aggregate the frame embeddings (e.g. average) to generate video embedding
Video-level models are computationally more expensive. Frame-level models are faster to train and serve, but it won't take into temporal aspects of the video. In practice, frame-level models are preferred in many cases. 

For the detailed structure of the frame-level models it can be CNN based framework.
### Putting them together
We use contrastive loss to train the video encoder and text encoder. It can look something like this
![[Screenshot 2025-04-02 at 8.42.00 PM.png]]
(Comment: I don't know why we do one video vs n query, I guess we can also do one query vs n videos)

## Evaluation
Assuming that we only have one video associated with one query in our evaluation set in our evaluation dataset.

To test, we input one query, and get its top videos (say top 10).
### Offline Evaluation
Since we only have one positive video for each query, we can look at:
- Recall@k: which is basically whether the positive video is in the top k list?
	- Cons: it doesn't take the position into account
- Mean Reciprocal Rank (MRR): averaging the rank of the first relevant item in each search result.
- Precision@k is not suitable here because we only have 1 positive video, so precision@k is at most 1/k.
### Online metrics
- CTR: click through rate
	- Pros: intuitive
	- Cons: people might click on the video even if it's not relevant
- Video completion rate
	- Cons: maybe someone didn't watch completely, but it is relevant
- Watch time of search results or things like watch time > 20% of the video time
## Serving
![[Screenshot 2025-04-02 at 8.54.40 PM.png]]