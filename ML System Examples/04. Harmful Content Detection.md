## Clarifying Requirements
- Business objective: reduce number of complaints
- What content modes? text, images and videos, and any combinations of these
- Do we have different category of harmful content: violence, nudity, hate speech etc.
- What data is available? We have a limited human-labeled data available about 10,000 per day. Users can also report harmful posts.
- Scale of the system? 500 million posts each day?
- Should we explain to a user why the content is removed? Yes
- Latency? Can we do batch predictions say hourly or daily? Or we need online prediction? For violent content, we want online-prediction, for other kinds, late prediction is fine.
**Problem statement** We want to develop a system to detect harmful content and remove them. When remove them, we need to explain to the user why it is harmful. There are different categories of harmful posts: violence, nudity, hate speech. The content can be text, video and images or combination of these and are of various languages. Users can report harmful posts.
## Frame it as an ML task
- ML objective: accurately harmful posts, and we need to provide the reason.
- I/O
![[Screenshot 2025-04-07 at 14.29.35.png]]
- ML category: classification model
## Data
- Labels
	- Hand label: accurate, but fairly small volume
	- Natural label: use user reports to label post
	- Non-harmful posts: sample from background distribution
- Data imbalance: we have less harmful content than non-harmful content. When doing sampling, we can sample similar amount of non-harmful posts as harmful posts.
- Features
	- Content
		- Text: text preprocessing, then use a pre-trained model to convert to embeddings
		- Images and videos: image or video preprocessing, then use pre-trained model to convert to embeddings
	- User interaction
		- User reaction: number of likes, shares, reports. These are numerical values, so we do some scaling.
		- Comments: use pre-trained model to convert them into embeddings, then aggregate them
	- Author information
		- Demographics
			- age
			- gender
			- country
		- Violation history
			- Number of violations
			- Total user reports
		- Activeness
			- Number of followers and followings
			- account age
![[Screenshot 2025-04-07 at 15.10.08.png]]
## Model development
- Model
	- How to handle multi-modal: we choose **early fuse** to handle text, image and videos, see [[Handle Multi-modal in Modeling]]
	- How to handle different harm categories
		- Method 0: Single binary classifier
			- Cons:
				- Can't inform users the reason why it's harmful
				- Not easy to identify harmful classes in which the system doesn't perform well
		- Method 1: One binary classifier per harmful class
			- Pros
				- We can inform users the reason why it's harmful
				- We can monitor and improve each model independently
			- Cons
				- Time consuming and expensive training
		- Method 2: Multi-label classifier
			- Pros: less costly
			- Cons: predicting the probabilities of each harmful class using a shared model isn't ideal because features may need to be transformed differently for each harmful class
		- Method 3: Multi-task classifier (see [[Different Type of Classification#Multi-task classification]]). We treat each harmful category as a different task. We have some shared layers to transform features, then we have task-specific layers, i.e. multiple classification head to finally make the prediction for each harmful category.
			- Pros
				- Not very costly to train
				- Shared layers are beneficial for each task
				- Training data for each task contributes to the learning of other tasks. Especially useful when data is limited
![[Pasted image 20250407203118.png]]

## Evaluation
### Offline evaluation
Since it's a classification problem, we can use common classification measurements, see [[Classification Evaluation Measures]]
### Online evaluation
- Prevalence: The ratio of harmful posts which we didn't prevent
$$ Prevalence = \frac{\text{Number of harmful posts we didn't prevent}}{\text{Total number of posts}}$$
Limitation: treats harmful posts equally. In real life, some posts have more views than others
- Harmful impressions: incorporate impression information of the posts
- Valid appeals: Percentage of posts that were deemed harmful, but appealed and reversed
$$Appeals = \frac{\text{Number of reversed appeals}}{\text{Number of harmful posts detected by the system}}$$
- Proactive rate: percentage of harmful posts found and deleted by the system before users report it
$$\text{Proactive rate} = \frac{\text{Number of harmful posts detected by the system}}{\text{Number of harmful posts detected by the system} + \text{ reported by users}} $$
- User reports per harmful class. Is this metric decreasing with the system?
## Serving
### Harmful content detection service
Given a new post, this service predicts the probability of harm. According to the requirements, some types of harm should be handled immediately due to their sensitivity. When this happens, the violation enforcement service removes the post immediately.
### Violation enforcement service
The violation enforcement service immediately takes down a post if the harmful content detection service predicts harm with high confidence. It also notifies the user why the post was removed.
### Demoting service
If the harmful content detection service predicts harm with low confidence, the demoting service temporarily demotes the post in order to decrease the chance of it spreading among users.

Then, the post is stored in storage for manual review by humans. The review team manually reviews the post and assigns a label from one of the predefined classes of harm. We will use these labeled posts in future training iterations to improve the model.