## Clarifying Requirements
- Business objective: reduce number of complaints
- What content modes? text, images and videos, and any combinations of these
- Do we have different category of harmful content: violence, nudity, hate speech etc.
- What data is available? We have a limited human-labeled data available about 10,000 per day. Users can also report harmful posts.
- Scale of the system? 500 million posts each day?
- Should we explain to a user why the content is removed? Yes
- Latency? Can we do batch predictions say hourly or daily? Or we need online prediction? For violent content, we want online-prediction, for other kinds, late prediction is fine.
**Problem statement** We want to develop a system to detect harmful content and remove them. When remove them, we need to explain to the user why it is harmful. There are different categories of harmful posts: violence, nudity, hate speech. The content can be text, video and images or combination of these and are of various languages. Users can report harmful posts.
## Frame it as an ML task
- ML objective: accurately harmful posts, and we need to provide the reason.
- I/O
![[Screenshot 2025-04-07 at 14.29.35.png]]
- ML category: classification model
## Data
- Labels
	- Hand label: accurate, but fairly small volume
	- Natural label: use user reports to label post
	- Non-harmful posts: sample from background distribution
- Data imbalance: we have less harmful content than non-harmful content. When doing sampling, we can sample similar amount of non-harmful posts as harmful posts.
- Features
	- Content
		- Text: text preprocessing, then use a pre-trained model to convert to embeddings
		- Images and videos: image or video preprocessing, then use pre-trained model to convert to embeddings
	- User interaction
		- User reaction: number of likes, shares, comments, reports. These are numerical values, so we do some scaling.
		- Comments: use pre-trained model to convert them into embeddings, then aggregate them
	- Author information
		- Demographics
			- age
			- gender
			- country
		- Violation history
			- Number of violations
			- Total user reports
		- Activeness
			- Number of followers and followings
			- account age
![[Screenshot 2025-04-07 at 15.10.08.png]]
## Model development
- Model
	- How to handle multi-modal: we choose early fuse to handle text, image and videos, see [[Handle Multi-modal in Modeling]]
	- How to handle different harm categories: we choose multi-task classification